# UIS Gitlab deployment

This repository contains a declarative deployment of Gitlab using the official
helm charts on GKE.

**IMPORTANT:** when first experimenting with this repository, it is *highly*
recommended that you make use of terraform
[workspaces](https://www.terraform.io/docs/state/workspaces.html) to create a
parallel deployment first. See the section below on "getting familiar with this
deployment".

## Known issues

This deployment is not yet complete. Known issues include:

* There is no configuration of email sending or receiving.
* The mechanism for having the Gitlab instance on a custom URL is untested.
* Making and restoring from backups require some manual steps.
* The Kubernetes namespace used is hard-coded to "gitlab" which makes it a
    little too easy to do something on production when you thought it was test.

## Bootstrap

> **ALWAYS** make sure you have the latest versions of terraform and helm
> installed. Especially with helm, earlier versions have bugs which are not
> worked around by this configuration.

1. Download [terraform](https://www.terraform.io/) and [helm](https://helm.sh/).
   Helm must be installed so that the ``helm`` command is available on the path.
2. Make sure that the ``kubectl`` command is installed and is on the path.
3. Generate a key for the Terraform Service account in the uis-automation-dm
   project:

    ```bash
    $ gcloud iam --project uis-automation-dm service-accounts \
        keys create secrets/terraform-admin-service-account-credentials.json \
        --iam-account terraform-admin@uis-automation-dm.iam.gserviceaccount.com
    ```
4. Make sure your local helm install is up-to-date and has the Gitlab repository
   configured:

    ```bash
    $ helm init --client-only
    $ helm repo add gitlab https://charts.gitlab.io/
    ```

## Deploy Gitlab

```bash
# Required only once
$ terraform init

# Update production deployment
$ terraform apply -target=module.production

# Update test deployment
$ terraform apply -target=module.test

# Update all the things
$ terraform apply
```

### Configure SAML authentication

The deployment automatically generates a key and self-signed certificate to
integrate with Raven's Shibboleth personality. Before you can authenticate using
Raven, you will need to add some metadata to the Raven metadata site. See the
"recipes" section below for more information.

### First ever deployment

> This section only applies if you are deploying completely from scratch. You
> may be doing this if you are making use of terraform's
> [workspace](https://www.terraform.io/docs/state/workspaces.html) feature.

If you are getting errors of the following form:

```
module.{...}.provider.google: google: could not find default credentials.
```

Terraform may have difficulty reconciling state the very first time a new
deployment is made. This is because the Google provider's credentials are
themselves generated by terraform and it may have difficulty bootstrapping
itself. You can help it out by manually creating the project first:

```bash
$ terraform apply -target=module.production.module.project
$ terraform apply -target=module.test.module.project
```

## Overview

The general idea with this deployment is that individual instances of Gitlab can
be deployed *n* times. The default configuration includes a production and test
instance but we should be able to "let a thousand Gitlabs bloom".

To that end most resources which share a global namespace have random names. For
example, all of the DNS entries for the releases have random names. Similarly,
the Google project ids have random elements. From looking at other people's
terraform examples this appears to be Very Much The Way To Do It.

The top-level [main.tf](main.tf) in this repository contains configuration for
two environments: ``test`` and ``production``.

## Hacking on this deployment

*PLEASE, PLEASE, PLEASE* make sure you use ``terraform fmt`` before committing.

## Recipes

This section contains some recipes which are useful when dealing with the
deployment.

### Get Gitlab URL and initial root password

The following will (on Linux) open your web browser at the production deployment
and copy the root password to the clipboard. Replace ``production`` with
``test`` for the test deployment.

```bash
$ xdg-open $(terraform output production_gitlab_url)
$ terraform output production_initial_root_password | xclip -i -sel clip
```

### Using kubectl/helm directly

A kubeconfig file suitable for connecting to the clusters created by this
deployment is available as an output. For example, to connect to the production
cluster:

```bash
$ export KUBECONFIG=$(mktemp ./secrets/kubeconfig.XXXXXX)
$ terraform output production_kubeconfig_content > "$KUBECONFIG"
$ kubectl -n gitlab get pod
$ helm ls
```

### Overriding the domain name for a particular environment

> This feature is unused at the moment and may change as we make use of it.

The [environment module](environment/) takes a variable named ``gitlab_domain``.
This can be used to override the domain name which is passed into the gitlab
config.

This variable just updates the gitlab configuration, it doesn't do any DNS
registration and so it is up to you to make sure that the domain name ends up at
the right IP. This could be by making it a CNAME to a host within the generated
DNS zone.

### Doing some large scale refactoring

If you want to do some large-scale experimentation with this deployment, you can
switch to a new terraform
[workspace](https://www.terraform.io/docs/state/workspaces.html) which will spin
up a brand new set of infrastructure.

### Fixing random helm errors

You may encounter errors which look like the following:

```
helm_release.gitlab: rpc error: code = Unavailable desc = transport is closing
```

This is a sign that the ``helm init`` command succeeded but the tiller pod is
not yet fully up. By the time you've read the message and found this entry in
the README, the pod is probably up so just go back to your terminal and press
"up" and "enter".

### Changing Gitlab configuration

Note that changing the gitlab configuration will *not* automatically restart the
pods. If configuration changes don't seem to be taking effect, restart the pods
manually via ``kubectl``.

### SAML metadata

SAML metadata suitable for adding to the Raven [metadata
application](https://metadata.raven.cam.ac.uk/) is auto-generated and available
at ``/users/auth/saml/metadata`` on the deployed site. Somewhat tediously, the
generated ``<md:KeyDescriptor use="signing">`` tag and its contents must be
copied and used to provide a ``<md:KeyDescriptor use="encryption">`` tag. For
some reason, the SAML metadata generator does not do this for you.

### Backups

Gitlab backups currently require that the storage buckets be available via a S3
compatible API. Unfortunately the GCS buckets needs some hoops jumped through to
be access via that API.

To perform a backup, you can follow the [Gitlab
instructions](https://gitlab.com/charts/gitlab/blob/master/doc/backup-restore/backup.md)
except that you will need to manually set the credentials to a set of developer
credentials which can be obtained from the [Google
console](https://console.cloud.google.com/storage/settings).

> See
> [Google's documentation](https://cloud.google.com/storage/docs/interoperability)
> for more details on this.

After logging in to the task-runner pod via ``kubectl exec``, you will need to
manually set the ``AWS_ACCESS_KEY_ID`` and ``AWS_SECRET_KEY`` environment
variables using the GCS developer keys. Since these credentials are tied to an
individual and not to a service account we cannot include them in the
deployment.

> If you are setting the variables using ``export AWS_ACCESS_KEY_ID=...``,
> recall that starting the command with a space will ensure it is not written to
> the command line history.

### Restoring one environment from another

Rather than backup up buckets to the tar file using the backup utility, one can
save some space by directly copying buckets if the backup is from environment
to environment rather than intended for disaster recovery.

Firstly, add ``--skip`` options to the ``backup-utility`` call above to skip
backing up buckets:

```bash
$ backup-utility --skip artifacts --skip lfs --skip packages --skip registry --skip uploads
```

After restoring the new instance, one may copy buckets using the ``gsutil``
command:

```bash
# Set the following based on the bucket names in the environments being copied.
$ export SOURCE_PREFIX=some-prefix-
$ export DESTINATION_PREFIX=some-prefix-
$ for i in artifacts lfs packages registry uploads; do \
    gsutil rsync -r gs://${SOURCE_PREFIX}${i}/ gs://${DESTINATION_PREFIX}/${i}/; \
done
```

The chief advantages of doing it this way are that a) cloud-to-cloud copying
within Google is very fast and b) it does not require that the files be
downloaded and then re-uploaded.

### Restarting pods on configuration change

Pods do not always restart after a configuration change. There is an
[issue](https://gitlab.com/charts/gitlab/issues/57) open to address this but, in
the meantime, the following command can be used to perform a staged restart of
the pods:

```bash
$ kubectl -n gitlab get pod | egrep 'unicorn|task-runner' | cut -d " " -f1 - \
    | xargs -n1 -p kubectl -n gitlab delete pod
```

The use of the `-p` flag to `xargs` will prompt before running each command.  To
avoid service outage, you should confirm that a pod to replace the one deleted
has fully started before proceeding to delete the next pod.

## Getting familiar with this deployment

This deployment is large and complex and is possibly best understood by
experimenting with it. One has two choices in this regard:

1. Experiment but only ever deploy the testing release.

2. Set up two entirely new Google projects.

If you want to go down the latter route, you can set up a new "workspace". A
"workspace" is an entirely parallel terraform state which is different from the
"default" workspace. In this parallel workspace you can try deploying your own
version of Gitlab.

You really should read and digest the entire section of the terraform manual on
workspaces first but the quick, quick version is:

```bash
$ terraform workspace new my-workspace  # choose a better name than this!
$ terraform init
$ terraform apply -target=module.production.module.project -target=module.test.module.project
$ terraform apply
```

## Setting up terraform admin service account

> This should only ever need to be done once. It is documented here for
> reference. See the [backend configuration](backend.tf) for how the account is
> used.

We make use of terraform's remote state backend. We configure the backend to
store the terraform state in a GCS bucket. We need to configure a terraform
admin serice account which is used to manage the contents of that bucket and to
create the actual GCP projects in the deployment. The terraform admin service
account is created in the following way:

```bash
$ gcloud iam --project uis-automation-dm service-accounts \
    create terraform-admin --display-name "Terraform admin account"
$ gcloud projects add-iam-policy-binding uis-automation-dm \
    --member serviceAccount:terraform-admin@uis-automation-dm.iam.gserviceaccount.com \
    --role roles/viewer
$ gcloud projects add-iam-policy-binding uis-automation-dm \
    --member serviceAccount:terraform-admin@uis-automation-dm.iam.gserviceaccount.com \
    --role roles/storage.admin
$ gcloud alpha resource-manager folders add-iam-policy-binding 497670463628 \
    --member serviceAccount:terraform-admin@uis-automation-dm.iam.gserviceaccount.com \
    --role roles/resourcemanager.projectCreator
$ gcloud projects add-iam-policy-binding uis-automation-infrastructure \
    --member serviceAccount:terraform-admin@uis-automation-dm.iam.gserviceaccount.com \
    --role roles/dns.admin
```

The terraform state bucket is created in the following way:

```bash
$ gsutil mb -p uis-automation-dm -l europe-west2 gs://uis-devops-terraform-state-you6phet
$ gsutil versioning set on gs://uis-devops-terraform-state-you6phet
```

Additionally terraform-admin@uis-automation-dm.iam.gserviceaccount.com is added
as a "Billing User" to the billing account.
